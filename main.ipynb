{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b482d7-66cb-4482-87a6-971ba4964e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import RobertaModel, RobertaTokenizer, TrainingArguments, Trainer, DataCollatorWithPadding, RobertaForSequenceClassification\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from datasets import load_dataset, Dataset, ClassLabel\n",
    "import pickle\n",
    "import wandb\n",
    "from lion_pytorch import Lion\n",
    "from transformers import get_scheduler\n",
    "from torch.optim import AdamW, Adam\n",
    "from datasets import logging as datasets_logging\n",
    "from transformers import logging as transformers_logging\n",
    "from config import structure_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a219708-5193-4cc1-a716-a37fd72c3227",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 16 # 16 / 32 / 64 \n",
    "max_step = 2500\n",
    "# should be less than 6000 in current setting  \n",
    "optim = 2\n",
    "# 1 -> Adam lr: 2e-5~3e-5;\n",
    "# 2 -> AdamW lr: 1e-4~5e-5~1e-5 weight_decay=0.01~0.05~0.1;\n",
    "# 3 -> Lion: 2e-4~1e-4~5e-5 weight_decay=0.01/0.02\n",
    "scheduler = \"cosine\"\n",
    "# \"cosine\" / \"linear\"\n",
    "lr = 5e-5\n",
    "w_decay = 2e-2 # 1e-2 ~ 5e-2\n",
    "epoch = 1\n",
    "\n",
    "model_structure = 1\n",
    "# ref: config.py\n",
    "\n",
    "exp_name = f\"s-{model_structure}-bs{bs}-optim{optim}-lr{lr}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e8be3f-caa4-4ccf-b7a6-21eb2f8c90c0",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd391cac-9c4f-48b8-8413-4e798a97a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_logging.set_verbosity_error()\n",
    "base_model = 'roberta-base'\n",
    "dataset = load_dataset('ag_news', split='train', cache_dir = './data')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('./roberta-base')\n",
    "def preprocess(examples):\n",
    "    tokenized = tokenizer(examples['text'], truncation=True, padding=True)\n",
    "    return tokenized\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True,  remove_columns=[\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "split_datasets = tokenized_dataset.train_test_split(test_size=24000)\n",
    "train_dataset = split_datasets['train']\n",
    "eval_dataset = split_datasets['test']\n",
    "\n",
    "# Extract the number of classes and their names\n",
    "num_labels = dataset.features['label'].num_classes\n",
    "class_names = dataset.features[\"label\"].names\n",
    "# the labels: ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "\n",
    "# Create an id2label mapping\n",
    "# We will need this for our classifier.\n",
    "id2label = {i: label for i, label in enumerate(class_names)}\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308016d-06cc-4466-a7ba-6934d1e786b6",
   "metadata": {},
   "source": [
    "# Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978df088-9687-4120-8889-60ec3bc5abfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers_logging.set_verbosity_error()\n",
    "layer_configs = structure_config(model_structure)\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    base_model,\n",
    "    id2label=id2label)\n",
    "# print(model)\n",
    "\n",
    "for config in layer_configs:\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        base_model,\n",
    "        id2label=id2label\n",
    "    )\n",
    "\n",
    "    target_layer = f\"encoder.layer.{config['layer']}.attention.self.query\"\n",
    "    peft_config = LoraConfig(\n",
    "        r=config[\"r\"],\n",
    "        lora_alpha=config[\"lora_alpha\"],\n",
    "        lora_dropout=config[\"lora_dropout\"],\n",
    "        bias=\"none\",\n",
    "        target_modules=config[\"target\"],\n",
    "        task_type=\"SEQ_CLS\",\n",
    "    )\n",
    "    print(f\"Applying LoRA to {target_layer}\")\n",
    "    model = get_peft_model(model, peft_config) \n",
    "\n",
    "peft_model = model\n",
    "print(peft_model)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5899e18a-ca26-4846-8a21-ba559f8158a1",
   "metadata": {},
   "source": [
    "# Training Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a85c279-6835-4af1-a64c-48951943bb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "if optim == 2:\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=w_decay)\n",
    "elif optim == 3:\n",
    "    optimizer = Lion(model.parameters(),lr=lr,weight_decay=w_decay)\n",
    "else:\n",
    "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=w_decay)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "# Setup Training args\n",
    "wandb.login()\n",
    "output_dir = \"results\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    report_to='wandb',\n",
    "    run_name = exp_name,\n",
    "    eval_strategy='steps',\n",
    "    logging_steps=100,\n",
    "    save_steps=100,\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model='accuracy',\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=lr,\n",
    "    num_train_epochs=epoch,\n",
    "    max_steps= max_step,\n",
    "    use_cpu=False,\n",
    "    dataloader_num_workers=64,\n",
    "    per_device_train_batch_size=bs, # total trained samples: batch_size*max_steps\n",
    "    per_device_eval_batch_size=256,\n",
    "    optim=\"sgd\", # ignore\n",
    "    gradient_checkpointing=False,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant':True}\n",
    ")\n",
    "\n",
    "scheduler = get_scheduler(\n",
    "    name=scheduler,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(0.1 * training_args.max_steps),  \n",
    "    num_training_steps=training_args.max_steps\n",
    ")\n",
    "# normally, num_training_steps = num_epochs * dataset_size / batch_size\n",
    "\n",
    "def get_trainer(model_):\n",
    "    early_stopping = EarlyStoppingCallback(\n",
    "        early_stopping_patience=10,  \n",
    "        early_stopping_threshold=0.001  \n",
    "    )\n",
    "    return  Trainer(\n",
    "      model=model_,\n",
    "      args=training_args,\n",
    "      compute_metrics=compute_metrics,\n",
    "      train_dataset=train_dataset,\n",
    "      eval_dataset=eval_dataset,\n",
    "      data_collator=data_collator,\n",
    "      optimizers=(optimizer, scheduler),\n",
    "      callbacks=[early_stopping])\n",
    "\n",
    "peft_lora_finetuning_trainer = get_trainer(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414e6a92-418a-431b-87a7-e9aa8f9c1ee6",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be33344-1d03-49de-b254-702f304ae0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = peft_lora_finetuning_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f2e9d-08d9-4ae3-a088-3e97018f4c88",
   "metadata": {},
   "source": [
    "# Local AGNEWS test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1182377-7205-4bf2-9519-404f88b21000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import importlib.util\n",
    "\n",
    "def evaluate_model(inference_model, dataset, labelled=True, batch_size=8, data_collator=None):\n",
    "    \"\"\"\n",
    "    Evaluate a PEFT model on a dataset.\n",
    "\n",
    "    Args:\n",
    "        inference_model: The model to evaluate.\n",
    "        dataset: The dataset (Hugging Face Dataset) to run inference on.\n",
    "        labelled (bool): If True, the dataset includes labels and metrics will be computed.\n",
    "                         If False, only predictions will be returned.\n",
    "        batch_size (int): Batch size for inference.\n",
    "        data_collator: Function to collate batches. If None, the default collate_fn is used.\n",
    "\n",
    "    Returns:\n",
    "        If labelled is True, returns a tuple (metrics, predictions)\n",
    "        If labelled is False, returns the predictions.\n",
    "    \"\"\"\n",
    "    # Create the DataLoader\n",
    "    eval_dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=data_collator)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    inference_model.to(device)\n",
    "    inference_model.eval()\n",
    "\n",
    "    all_predictions = []\n",
    "    if labelled:\n",
    "        module_path = \"./metrics/accuracy.py\"\n",
    "        spec = importlib.util.spec_from_file_location(\"accuracy\", module_path)\n",
    "        accuracy = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(accuracy)\n",
    "        metric = accuracy.Accuracy()\n",
    "        # metric = evaluate.load('./metrics', module_type=\"metric\")\n",
    "\n",
    "    # Loop over the DataLoader\n",
    "    for batch in tqdm(eval_dataloader, mininterval=0.5):\n",
    "        # Move each tensor in the batch to the device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = inference_model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        all_predictions.append(predictions.cpu())\n",
    "\n",
    "        if labelled:\n",
    "            # Expecting that labels are provided under the \"labels\" key.\n",
    "            references = batch[\"labels\"]\n",
    "            metric.add_batch(\n",
    "                predictions=predictions.cpu().numpy(),\n",
    "                references=references.cpu().numpy()\n",
    "            )\n",
    "\n",
    "    # Concatenate predictions from all batches\n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "\n",
    "    if labelled:\n",
    "        eval_metric = metric.compute()\n",
    "        print(\"Evaluation Metric:\", eval_metric)\n",
    "        return eval_metric, all_predictions\n",
    "    else:\n",
    "        return all_predictions\n",
    "\n",
    "best_model = peft_lora_finetuning_trainer.model\n",
    "_, _ = evaluate_model(best_model, eval_dataset, True, 256, data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcd1402-dfdf-4350-bec3-931f27aed195",
   "metadata": {},
   "source": [
    "# Kaggle Testset Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63419a93-3ffe-4920-891a-0e8526800608",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_dataset = pd.read_pickle(\"test_unlabelled.pkl\")\n",
    "test_dataset = unlabelled_dataset.map(preprocess, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Run inference and save predictions\n",
    "preds = evaluate_model(best_model, test_dataset, False, 256, data_collator)\n",
    "df_output = pd.DataFrame({\n",
    "    'ID': range(len(preds)),\n",
    "    'Label': preds.numpy()  # or preds.tolist()\n",
    "})\n",
    "df_output.to_csv(os.path.join(output_dir,\"inference_output.csv\"), index=False)\n",
    "print(\"Inference complete. Predictions saved to inference_output.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
